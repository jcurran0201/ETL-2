{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import re\n",
    "import dateutil\n",
    "from sqlalchemy import create_engine\n",
    "import psycopg2\n",
    "from config import db_password"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_string = f\"postgresql://postgres:{db_password}@127.0.0.1:5432/movies_data\" \n",
    "\n",
    "engine = create_engine(db_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_dollars(s):  \n",
    "    s = str(s)\n",
    "    match = re.search(r\"\\$([\\d,.]+)[^\\d,.]?.*([mb])il\",s) \n",
    "    if not match: \n",
    "        return np.nan \n",
    "    digit = float(match[1].replace(\",\",\"\"))\n",
    "    try: \n",
    "        multiplier=match[2]  \n",
    "    except IndexError: \n",
    "        multiplier = 1.0 \n",
    "    else: \n",
    "        multiplier = {\"m\":1e6, \"b\":1e9}[multiplier] \n",
    "    return digit*multiplier \n",
    "\n",
    "def parse_date(s): \n",
    "    if not isinstance(s,list): \n",
    "        s=[s] \n",
    "    for i in s: \n",
    "        try: \n",
    "            date = pd.to_datetime(i) \n",
    "        except: \n",
    "            continue \n",
    "        else: \n",
    "            return date \n",
    "        return np.nan \n",
    "    \n",
    "def parse_time(s): \n",
    "    s = str(s) \n",
    "    m = re.search(r\"(\\d+)\", s) \n",
    "    if not m: \n",
    "        return np.nan \n",
    "    else: \n",
    "        return pd.to_timedelta(float(m[1]),\"min\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Add the clean movie function that takes in the argument, \"movie\".\n",
    "def clean_movie(movie):\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    return movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wkm():\n",
    "    # 2. Read in the kaggle metadata and MovieLens ratings CSV files as Pandas DataFrames.\n",
    "    kaggle_metadata = pd.read_csv(kaggle_file) \n",
    "    ratings = pd.read_csv(ratings_file) \n",
    "    \n",
    "    # 3. Open the read the Wikipedia data JSON file.\n",
    "    with open (wiki_file,'r') as f: \n",
    "        wiki_movies = json.load(f) \n",
    "    wiki_movies = [entry for entry in wiki_movies if \"No. of episodes\" not in entry] \n",
    "    wiki_movies = [clean_movie(entry) for entry in wiki_movies]\n",
    "\n",
    "    wiki_movies_with_id = []\n",
    "    imdb_ids = set()\n",
    "    for entry in wiki_movies:  \n",
    "        try: \n",
    "            m = re.search(r\"title/(tt\\d{7,8})/\", entry[\"imdb_link\"]) \n",
    "            imdb_id = m[1]\n",
    "        except Exception as e:  \n",
    "            print(f\"{entry.get('title', 'N/A')}: {e}\")\n",
    "            continue\n",
    "        if imdb_id in imdb_ids: \n",
    "            continue \n",
    "        entry[\"imdb_id\"] = imdb_id\n",
    "        imdb_ids.add(imdb_id)\n",
    "        wiki_movies_with_id.append(entry)\n",
    "        \n",
    "    wiki_movies_df = pd.DataFrame(wiki_movies_with_id)  \n",
    "    wiki_movies_df = wiki_movies_df.dropna(how='all',axis=1)\n",
    "    #box_office = wiki_movies_df[\"Box office\"].dropna()\n",
    "    wiki_movies_df[\"Box office\"] = wiki_movies_df[\"Box office\"].apply(parse_dollars) \n",
    "    wiki_movies_df[\"Release date\"] = wiki_movies_df[\"Release date\"].apply(parse_date) \n",
    "    wiki_movies_df[\"Running time\"] = wiki_movies_df[\"Running time\"].apply(parse_time)  \n",
    "    \n",
    "    kaggle_metadata = kaggle_metadata[kaggle_metadata['adult'] == 'False'].drop('adult',axis='columns') \n",
    "    kaggle_metadata['video'] = kaggle_metadata['video'] == 'True' \n",
    "    kaggle_metadata['budget'] = kaggle_metadata['budget'].astype(int)\n",
    "    kaggle_metadata['id'] = pd.to_numeric(kaggle_metadata['id'], errors='raise')\n",
    "    kaggle_metadata['popularity'] = pd.to_numeric(kaggle_metadata['popularity'], errors='raise') \n",
    "    kaggle_metadata['release_date'] = pd.to_datetime(kaggle_metadata['release_date']) \n",
    "    # 5. Return the three DataFrames\n",
    "    # 5. Return the three DataFrames\n",
    "    \n",
    "    movies_df = pd.merge(wiki_movies_df, kaggle_metadata, on='imdb_id', suffixes=['_wiki','_kaggle'], how= \"inner\")  \n",
    "    #Drop useless columns that clutter\n",
    "    movies_df.drop(columns=\n",
    "        [ \"Polish\", \"Chinese\", \"Yiddish\", \"Arabic\", 'Hebrew', \"Russian\", \"Cantonese\"\n",
    "        , \"Japanese\", \"McCuneâ€“Reischauer\", \"Revised Romanization\", \"Hangul\", \"French\", \"Mandarin\"\n",
    "        ], inplace=True)\n",
    "    movies_df.drop(columns=[\"Hepburn\",\"Species\"], inplace=True) \n",
    "    #Drop columns required by step 5 \n",
    "    movies_df.drop(columns=[\"Original language(s)\",\"Original language\",\"Language\"], inplace=True)\n",
    "    movies_df.drop(columns=['Production company(s)', 'Productioncompanies ', 'Productioncompany '], inplace=True)\n",
    "    movies_df.drop(columns=['Release date'], inplace=True) \n",
    "    movies_df.drop(columns=['Original title', 'title_wiki'], inplace=True) \n",
    "\n",
    "    def fill_missing_kaggle_data(df, kaggle_column, wiki_column):\n",
    "        df[kaggle_column] = df.apply(\n",
    "            lambda row: row[wiki_column] if row[kaggle_column] == 0 else row[kaggle_column]\n",
    "            , axis=1)\n",
    "        df.drop(columns=[wiki_column], inplace=True)\n",
    "\n",
    "    fill_missing_kaggle_data(movies_df, 'revenue', 'Box office')\n",
    "    fill_missing_kaggle_data(movies_df, 'budget', 'Budget') \n",
    "    fill_missing_kaggle_data(movies_df, 'runtime', 'Running time')\n",
    "\n",
    "\n",
    "\n",
    "    movies_df = (movies_df.loc[:, \n",
    "        [\"imdb_id\",'id','title_kaggle','original_title','tagline','belongs_to_collection', 'url','imdb_link','runtime', \n",
    "         'budget','revenue','release_date','popularity','vote_average', 'vote_count','genres','original_language', \n",
    "         'overview','spoken_languages','Country','production_companies','production_countries','Distributor','Producer(s)',\n",
    "        'Director','Starring','Cinematography','Editor(s)','Written by','Composer(s)','Based on']])\n",
    "\n",
    "    movies_df.rename({'id':'kaggle_id',\n",
    "                      'title_kaggle':'title',\n",
    "                      'url':'wikipedia_url',\n",
    "                      'Country':'country',\n",
    "                      'Distributor':'distributor',\n",
    "                      'Producer(s)':'producers',\n",
    "                      'Director':'director',\n",
    "                      'Starring':'starring',\n",
    "                      'Cinematography':'cinematography',\n",
    "                      'Editor(s)':'editors',\n",
    "                      'Written by':'writers',\n",
    "                      'Composer(s)':'composers',\n",
    "                      'Based on':'based_on'\n",
    "                     }, axis='columns', inplace=True)\n",
    "\n",
    "    rating_counts = ratings.groupby(['movieId','rating'], as_index=False).count() \\\n",
    "                    .rename({'userId':'count'}, axis=1) \\\n",
    "                    .pivot(index='movieId',columns='rating', values='count')\n",
    "\n",
    "    rating_counts.columns = ['rating_' + str(col) for col in rating_counts.columns]  \n",
    "    movies_with_ratings_df = pd.merge(movies_df, rating_counts, left_on='kaggle_id', right_index=True, how='left')\n",
    "    movies_with_ratings_df[rating_counts.columns] = movies_with_ratings_df[rating_counts.columns].fillna(0)\n",
    "\n",
    "    \n",
    "    #return wiki_movies_df, kaggle_metadata, ratings\n",
    "    #return wiki_movies_df, movies_with_ratings_df, rating_counts\n",
    "    \n",
    "    # Return\n",
    "    #  * wiki file (cleaned, un-merged)\n",
    "    #  * wiki file (cleaned) + kaggle file (cleaned) + ratings file (cleaned)\n",
    "    #  * wiki file (cleaned) + kaggle file (cleaned)\n",
    "    #return wiki_movies_df, movies_with_ratings_df, movies_df\n",
    "\n",
    "    # Return\n",
    "    #  * wiki file (cleaned, un-merged)\n",
    "    #  * wiki file (cleaned) + kaggle file (cleaned)\n",
    "    #  * wiki file (cleaned) + kaggle file (cleaned) + ratings file (cleaned)\n",
    "    #  * kaggle_file (cleaned)\n",
    "    #  * ratings file (cleaned)\n",
    "    #return wiki_movies_df, movies_df, movies_with_ratings_df, kaggle_metadata, rating_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_dir = \"//Users/johncurran/Desktop/Rutgers Data Sci Bootcamp/Challenges/ETL Challenge (DONE)/archive\"\n",
    "# Wikipedia data\n",
    "wiki_file = f'{file_dir}/wikipedia-movies.json'\n",
    "# Kaggle metadata\n",
    "kaggle_file = f'{file_dir}/movies_metadata.csv'\n",
    "# MovieLens rating data.\n",
    "ratings_file = f'{file_dir}/ratings.csv'\n",
    "\n",
    "# 7. Set the three variables in Step 6 equal to the function created in Step 1.\n",
    "wiki_file, kaggle_file, ratings_file = wkm() #=extract_transform_load() \n",
    "#wiki_file, wiki_kaggle_file, wiki_kaggle_ratings_file, kaggle_file, ratings_file = wkm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 12. Set the DataFrames from the return statement equal to the file names in Step 11. \n",
    "wiki_movies_df = wiki_file\n",
    "movies_with_ratings_df = kaggle_file\n",
    "movies_df = ratings_file "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing rows 0 to 1000000...Done.\n",
      "importing rows 1000000 to 2000000...Done.\n",
      "importing rows 2000000 to 3000000...Done.\n",
      "importing rows 3000000 to 4000000...Done.\n",
      "importing rows 4000000 to 5000000...Done.\n",
      "importing rows 5000000 to 6000000...Done.\n",
      "importing rows 6000000 to 7000000...Done.\n",
      "importing rows 7000000 to 8000000...Done.\n",
      "importing rows 8000000 to 9000000...Done.\n",
      "importing rows 9000000 to 10000000...Done.\n",
      "importing rows 10000000 to 11000000...Done.\n",
      "importing rows 11000000 to 12000000...Done.\n",
      "importing rows 12000000 to 13000000...Done.\n",
      "importing rows 13000000 to 14000000...Done.\n",
      "importing rows 14000000 to 15000000...Done.\n",
      "importing rows 15000000 to 16000000...Done.\n",
      "importing rows 16000000 to 17000000...Done.\n",
      "importing rows 17000000 to 18000000...Done.\n",
      "importing rows 18000000 to 19000000...Done.\n",
      "importing rows 19000000 to 20000000...Done.\n",
      "importing rows 20000000 to 21000000...Done.\n",
      "importing rows 21000000 to 22000000...Done.\n",
      "importing rows 22000000 to 23000000...Done.\n",
      "importing rows 23000000 to 24000000...Done.\n",
      "importing rows 24000000 to 25000000...Done.\n",
      "importing rows 25000000 to 26000000...Done.\n",
      "importing rows 26000000 to 26024289...Done.\n",
      "importing rows 0 to 1000000...Done. 37.01086091995239 total seconds elapsed\n",
      "importing rows 1000000 to 2000000...Done. 73.57066893577576 total seconds elapsed\n",
      "importing rows 2000000 to 3000000...Done. 108.10697078704834 total seconds elapsed\n",
      "importing rows 3000000 to 4000000...Done. 145.28023982048035 total seconds elapsed\n",
      "importing rows 4000000 to 5000000...Done. 181.96090269088745 total seconds elapsed\n",
      "importing rows 5000000 to 6000000...Done. 218.22767162322998 total seconds elapsed\n",
      "importing rows 6000000 to 7000000...Done. 255.2261607646942 total seconds elapsed\n",
      "importing rows 7000000 to 8000000...Done. 292.82678484916687 total seconds elapsed\n",
      "importing rows 8000000 to 9000000...Done. 330.2292289733887 total seconds elapsed\n",
      "importing rows 9000000 to 10000000...Done. 367.3675329685211 total seconds elapsed\n",
      "importing rows 10000000 to 11000000...Done. 403.6165838241577 total seconds elapsed\n",
      "importing rows 11000000 to 12000000...Done. 441.6624128818512 total seconds elapsed\n",
      "importing rows 12000000 to 13000000...Done. 2259.2395668029785 total seconds elapsed\n",
      "importing rows 13000000 to 14000000...Done. 2387.652020931244 total seconds elapsed\n",
      "importing rows 14000000 to 15000000...Done. 2501.7309687137604 total seconds elapsed\n",
      "importing rows 15000000 to 16000000...Done. 2583.230038881302 total seconds elapsed\n",
      "importing rows 16000000 to 17000000...Done. 2665.1410789489746 total seconds elapsed\n",
      "importing rows 17000000 to 18000000...Done. 2747.579304933548 total seconds elapsed\n",
      "importing rows 18000000 to 19000000...Done. 2827.3267986774445 total seconds elapsed\n",
      "importing rows 19000000 to 20000000...Done. 3885.5637109279633 total seconds elapsed\n",
      "importing rows 20000000 to 21000000...Done. 3980.448278903961 total seconds elapsed\n",
      "importing rows 21000000 to 22000000...Done. 4253.278559923172 total seconds elapsed\n",
      "importing rows 22000000 to 23000000...Done. 5082.006905794144 total seconds elapsed\n",
      "importing rows 23000000 to 24000000...Done. 5165.974808692932 total seconds elapsed\n",
      "importing rows 24000000 to 25000000...Done. 5236.77507686615 total seconds elapsed\n",
      "importing rows 25000000 to 26000000...Done. 5302.919509887695 total seconds elapsed\n",
      "importing rows 26000000 to 26024289...Done. 5304.496209859848 total seconds elapsed\n"
     ]
    }
   ],
   "source": [
    "# Do Not run yet! STEP 1\n",
    "rows_imported = 0\n",
    "for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    print(f'Done.') \n",
    "# STEP 2 Print Elapsed Time\n",
    "rows_imported = 0\n",
    "# get the start_time from time.time()\n",
    "start_time = time.time()\n",
    "for data in pd.read_csv(f'{file_dir}/ratings.csv', chunksize=1000000):\n",
    "    print(f'importing rows {rows_imported} to {rows_imported + len(data)}...', end='')\n",
    "    data.to_sql(name='ratings', con=engine, if_exists='append')\n",
    "    rows_imported += len(data)\n",
    "\n",
    "    # add elapsed time to final print out\n",
    "    print(f'Done. {time.time() - start_time} total seconds elapsed')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/9s/jwyj784906ndy4td5h3gqtnw0000gn/T/ipykernel_8517/1906108907.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# 13. Check the wiki_movies_df DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mwiki_movies_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "# 13. Check the wiki_movies_df DataFrame. \n",
    "wiki_movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 14. Check the movies_with_ratings_df DataFrame.\n",
    "movies_with_ratings_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 15. Check the movies_df DataFrame. \n",
    "movies_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
